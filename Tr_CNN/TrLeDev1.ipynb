{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a2a5197-217c-4a58-9b04-320ec6b934dd",
   "metadata": {},
   "source": [
    "# Transfer Learning in Keras with Computer Vision Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446566d0-1e1f-4f8b-adaa-3c65944f91aa",
   "metadata": {},
   "source": [
    "These models can be used as the basis for transfer learning in computer vision applications.\n",
    "\n",
    "This is desirable for a number of reasons, not least:\n",
    "\n",
    "- Useful Learned Features: The models have learned how to detect generic features from photographs, given that they were trained on more than 1,000,000 images for 1,000 categories.\n",
    "- State-of-the-Art Performance: The models achieved state of the art performance and remain effective on the specific image recognition task for which they were developed.\n",
    "- Easily Accessible: The model weights are provided as free downloadable files and many libraries provide convenient APIs to download and use the models directly.\n",
    "\n",
    "The model weights can be downloaded and used in the same model architecture using a range of different deep learning libraries, including Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62c3687-8ae4-4687-bc63-65aca279110d",
   "metadata": {},
   "source": [
    "## How to Use Pre-Trained Models\n",
    "\n",
    "The use of a pre-trained model is limited only by your creativity.\n",
    "\n",
    "For example, a model may be downloaded and used as-is, such as embedded into an application and used to classify new photographs.\n",
    "\n",
    "Alternately, models may be downloaded and use as feature extraction models. Here, the output of the model from a layer prior to the output layer of the model is used as input to a new classifier model.\n",
    "\n",
    "Recall that convolutional layers closer to the input layer of the model learn low-level features such as lines, that layers in the middle of the layer learn complex abstract features that combine the lower level features extracted from the input, and layers closer to the output interpret the extracted features in the context of a classification task.\n",
    "\n",
    "Armed with this understanding, a level of detail for feature extraction from an existing pre-trained model can be chosen. For example, if a new task is quite different from classifying objects in photographs (e.g. different to ImageNet), then perhaps the output of the pre-trained model after the few layers would be appropriate. If a new task is quite similar to the task of classifying objects in photographs, then perhaps the output from layers much deeper in the model can be used, or even the output of the fully connected layer prior to the output layer can be used.\n",
    "\n",
    "The pre-trained model can be used as a separate feature extraction program, in which case input can be pre-processed by the model or portion of the model to a given an output (e.g. vector of numbers) for each input image, that can then use as input when training a new model.\n",
    "\n",
    "Alternately, the pre-trained model or desired portion of the model can be integrated directly into a new neural network model. In this usage, the weights of the pre-trained can be frozen so that they are not updated as the new model is trained. Alternately, the weights may be updated during the training of the new model, perhaps with a lower learning rate, allowing the pre-trained model to act like a weight initialization scheme when training the new model.\n",
    "\n",
    "We can summaries some of these usage patterns as follows:\n",
    "\n",
    "\n",
    "- **Classifier**: The pre-trained model is used directly to classify new images.\n",
    "- **Standalone Feature Extractor:** The pre-trained model, or some portion of the model, is used to pre-process images and extract relevant features.\n",
    "- **Integrated Feature Extractor:** The pre-trained model, or some portion of the model, is integrated into a new model, but layers of the pre-trained model are frozen during training.\n",
    "- **Weight Initialization:** The pre-trained model, or some portion of the model, is integrated into a new model, and the layers of the pre-trained model are trained in concert with the new model.\n",
    "\n",
    "Each approach can be effective and save significant time in developing and training a deep convolutional neural network model.\n",
    "\n",
    "It may not be clear as to which usage of the pre-trained model may yield the best results on your new computer vision task, therefore some experimentation may be required.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f9f062-3510-4333-b1cc-c6f8d12a046f",
   "metadata": {},
   "source": [
    "## Models for Transfer Learning\n",
    "\n",
    "Perhaps three of the more popular models are as follows:\n",
    "\n",
    "- VGG (e.g. VGG16 or VGG19).\n",
    "- GoogLeNet (e.g. InceptionV3).\n",
    "- Residual Network (e.g. ResNet50).\n",
    "\n",
    "These models are both widely used for transfer learning both because of their performance, but also because they were examples that introduced specific architectural innovations, namely consistent and repeating structures (VGG), inception modules (GoogLeNet), and residual modules (ResNet).\n",
    "\n",
    "Having [application API](https://keras.io/api/applications/)  for  system and include functions to load a model with or without the pre-trained weights, and prepare data in a way that a given model may expect (e.g. scaling of size and pixel values).\n",
    "\n",
    "When loading a given model, the “include_top” argument can be set to False, in which case the fully-connected output layers of the model used to make predictions is not loaded, allowing a new output layer to be added and trained. \n",
    "```python\n",
    "...\n",
    "# load model without output layer\n",
    "model = VGG16(include_top=False)\n",
    "```\n",
    "Additionally, when the “include_top” argument is False, the “input_tensor” argument must be specified, allowing the expected fixed-sized input of the model to be changed. \n",
    "\n",
    "```python\n",
    "...\n",
    "# load model and specify a new input shape for images\n",
    "new_input = Input(shape=(640, 480, 3))\n",
    "model = VGG16(include_top=False, input_tensor=new_input)\n",
    "```\n",
    "A model without a top will output activations from the last convolutional or pooling layer directly. One approach to summarizing these activations for thier use in a classifier or as a feature vector representation of input is to add a global pooling layer, such as a max global pooling or average global pooling. The result is a vector that can be used as a feature descriptor for an input. Keras provides this capability directly via the ‘pooling‘ argument that can be set to ‘avg‘ or ‘max‘. For example:\n",
    "```python\n",
    "# load model and specify a new input shape for images and avg pooling output\n",
    "new_input = Input(shape=(640, 480, 3))\n",
    "model = VGG16(include_top=False, input_tensor=new_input, pooling='avg')\n",
    "```\n",
    "Images can be prepared for a given model using the preprocess_input() function; e.g., pixel scaling is performed in a way that was performed to images in the training dataset when the model was developed.\n",
    "```python\n",
    "...\n",
    "# prepare an image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "images = ...\n",
    "prepared_images = preprocess_input(images)\n",
    "```\n",
    "\n",
    "Finally, you may wish to use a model architecture on your dataset, but not use the pre-trained weights, and instead initialize the model with random weights and train the model from scratch.\n",
    "\n",
    "This can be achieved by setting the ‘weights‘ argument to None instead of the default ‘imagenet‘. Additionally, the ‘classes‘ argument can be set to define the number of classes in your dataset, which will then be configured for you in the output layer of the model.\n",
    "```python \n",
    "...\n",
    "# define a new model with random weights and 10 classes\n",
    "new_input = Input(shape=(640, 480, 3))\n",
    "model = VGG16(weights=None, input_tensor=new_input, classes=10)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc52441c-264f-4bc4-afd4-3e282a29ee20",
   "metadata": {},
   "source": [
    "## Load the VGG16 Pre-trained Model\n",
    "\n",
    "By default, the model expects color input images to be rescaled to the size of 224×224 squares.\n",
    "\n",
    "```python\n",
    "# example of loading the vgg16 model\n",
    "from keras.applications.vgg16 import VGG16\n",
    "# load model\n",
    "model = VGG16()\n",
    "# summarize the model\n",
    "model.summary()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6341c7a2-86a9-4338-a8de-5832c90ec62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 4096)              102764544 \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 4096)              16781312  \n",
      "                                                                 \n",
      " predictions (Dense)         (None, 1000)              4097000   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "# load model\n",
    "model = VGG16()\n",
    "# summarize the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e25d745-eaec-424a-bf92-db4f22e7801f",
   "metadata": {},
   "source": [
    "\n",
    "## Load the InceptionV3 Pre-Trained Model\n",
    "\n",
    "The model expects color images to have the square shape 299×299.\n",
    "\n",
    "```python\n",
    "# example of loading the inception v3 model\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "# load model\n",
    "model = InceptionV3()\n",
    "# summarize the model\n",
    "model.summary()\n",
    "```\n",
    "\n",
    "## Load the ResNet50 Pre-trained Model\n",
    "\n",
    "The model expects color images to have the square shape 224×224.\n",
    "\n",
    "```python\n",
    "# example of loading the resnet50 model\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "# load model\n",
    "model = ResNet50()\n",
    "# summarize the model\n",
    "model.summary()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13f99b6-cbb6-4788-9ef0-c7d09f0fe631",
   "metadata": {},
   "source": [
    "## Examples of Using Pre-Trained Models\n",
    "\n",
    "- [Photograph of a Dog (dog.jpg)](https://machinelearningmastery.com/wp-content/uploads/2019/02/dog.jpg)\n",
    "\n",
    "### Pretraining model as Classifier\n",
    "A pre-trained model can be used directly to classify new photographs as one of the 1,000 known classes in the image classification task in the ILSVRC.\n",
    "\n",
    "First, the photograph needs to loaded and reshaped to a 224×224 square, expected by the model, and the pixel values scaled in the way expected by the model. The model operates on an array of samples, therefore the dimensions of a loaded image need to be expanded by 1, for one image with 224×224 pixels and three channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07c3b67b-2f5f-4e1a-898b-0d9db86003bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doberman 36.75\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input, decode_predictions\n",
    "\n",
    "image = load_img(\"Image/dog.jpg\", target_size=(224, 224))\n",
    "image = img_to_array(image)\n",
    "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "image = preprocess_input(image)\n",
    "\n",
    "model = VGG16()\n",
    "yhat = model.predict(image)\n",
    "lable = decode_predictions(yhat)\n",
    "lable = lable[0][0]\n",
    "print(\"{} {:.2f}\".format(lable[1], lable[2]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3005276-4b70-40f9-bd44-d62db69b08ff",
   "metadata": {},
   "source": [
    "## Pre-Trained Model as Feature Extractor Preprocessor\n",
    "The pre-trained model may be used as a standalone program to extract features from new photographs.\n",
    "\n",
    "Specifically, the extracted features of a photograph may be a vector of numbers that the model will use to describe the specific features in a photograph. These features can then be used as input in the development of a new model.\n",
    "\n",
    "The last few layers of the VGG16 model are fully connected layers prior to the output layer. These layers will provide a complex set of features to describe a given input image and may provide useful input when training a new model for image classification or related computer vision task.\n",
    "\n",
    "We will load the model with the classifier output part of the model, but manually remove the final output layer. This means that the second last fully connected layer with 4,096 nodes will be the new output layer.\n",
    "\n",
    "```python \n",
    "# load model\n",
    "model = VGG16()\n",
    "# remove the output layer\n",
    "model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "```\n",
    "This vector of 4,096 numbers will be used to represent the complex features of a given input image that can then be saved to file to be loaded later and used as input to train a new model. We can save it as [a pickle file](https://docs.python.org/3/library/pickle.html).\n",
    "```python\n",
    "# get extracted features\n",
    "features = model.predict(image)\n",
    "print(features.shape)\n",
    "# save to file\n",
    "dump(features, open('dog.pkl', 'wb'))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60881c3b-6e0d-4a83-87ab-476f0141322e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4096)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input, decode_predictions\n",
    "from tensorflow.keras.models import Model\n",
    "from pickle import dump\n",
    "\n",
    "image = load_img(\"Image/dog.jpg\", target_size=(224, 224))\n",
    "image = img_to_array(image)\n",
    "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "image = preprocess_input(image)\n",
    "\n",
    "model = VGG16()\n",
    "model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "features = model.predict(image)\n",
    "print(features.shape)\n",
    "# save to file\n",
    "dump(features, open('dog.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c736dedd-d215-44ec-9e82-16f4d344694f",
   "metadata": {},
   "source": [
    "Running the example loads the photograph, then prepares the model as a feature extraction model.\n",
    "\n",
    "The features are extracted from the loaded photo and the shape of the feature vector is printed, showing it has 4,096 numbers. This feature is then saved to a new file dog.pkl in the current working directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9e69e3-065b-41c7-b13f-a8bf4dcb87cc",
   "metadata": {},
   "source": [
    "## Pre-Trained Model as Feature Extractor in Model\n",
    "\n",
    "This can be achieved by loading the model, then simply adding new layers. This may involve adding new convolutional and pooling layers to expand upon the feature extraction capabilities of the model or adding new fully connected classifier type layers to learn how to interpret the extracted features on a new dataset, or some combination.\n",
    "\n",
    "For example, we can load the VGG16 models without the classifier part of the model by specifying the “include_top” argument to “False”, and specify the preferred shape of the images in our new dataset as 300×300.\n",
    "\n",
    "```python\n",
    "\n",
    "# load model without classifier layers\n",
    "model = VGG16(include_top=False, input_shape=(300, 300, 3))\n",
    "```\n",
    "We can then use the Keras function API to add a new Flatten layer after the last pooling layer in the VGG16 model, then define a new classifier model with a Dense fully connected layer and an output layer that will predict the probability for 10 classes.\n",
    "\n",
    "```python\n",
    "# add new classifier layers\n",
    "flat1 = Flatten()(model.layers[-1].output)\n",
    "class1 = Dense(1024, activation='relu')(flat1)\n",
    "output = Dense(10, activation='softmax')(class1)\n",
    "# define new model\n",
    "model = Model(inputs=model.inputs, outputs=output)\n",
    "```\n",
    "An alternative approach to adding a Flatten layer would be to define the VGG16 model with an average pooling layer, and then add fully connected layers. Perhaps try both approaches on your application and see which results in the best performance.\n",
    "\n",
    "The weights of the VGG16 model and the weights for the new model will all be trained together on the new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f489c45d-36d6-4531-b83a-7a4f00f69de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 300, 300, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 300, 300, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 300, 300, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 150, 150, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 150, 150, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 150, 150, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 75, 75, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 75, 75, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 75, 75, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 75, 75, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 37, 37, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 37, 37, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 37, 37, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 37, 37, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 18, 18, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 18, 18, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 18, 18, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 18, 18, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 9, 9, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 41472)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              42468352  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                10250     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 57,193,290\n",
      "Trainable params: 57,193,290\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.models import Model \n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "\n",
    "model = VGG16(include_top=False, input_shape=(300, 300, 3))\n",
    "flat1 = Flatten()(model.layers[-1].output)\n",
    "class1 = Dense(1024, activation='relu')(flat1)\n",
    "output = Dense(10, activation='softmax')(class1)\n",
    "\n",
    "model = Model(inputs=model.inputs, outputs=output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c82d5d-e3e7-477c-a749-25ef005108fe",
   "metadata": {},
   "source": [
    "Alternately, we may wish to use the VGG16 model layers, but train the new layers of the model without updating the weights of the VGG16 layers. This will allow the new output layers to learn to interpret the learned features of the VGG16 model.\n",
    "\n",
    "This can be achieved by setting the “trainable” property on each of the layers in the loaded VGG model to False prior to training. For example:\n",
    "```python\n",
    "# load model without classifier layers\n",
    "model = VGG16(include_top=False, input_shape=(300, 300, 3))\n",
    "# mark loaded layers as not trainable\n",
    "for layer in model.layers:\n",
    "\tlayer.trainable = False\n",
    "...\n",
    "```\n",
    "You can pick and choose which layers are trainable.\n",
    "\n",
    "For example, perhaps you want to retrain some of the convolutional layers deep in the model, but none of the layers earlier in the model. For example:\n",
    "\n",
    "```python\n",
    "# load model without classifier layers\n",
    "model = VGG16(include_top=False, input_shape=(300, 300, 3))\n",
    "# mark some layers as not trainable\n",
    "model.get_layer('block1_conv1').trainable = False\n",
    "model.get_layer('block1_conv2').trainable = False\n",
    "model.get_layer('block2_conv1').trainable = False\n",
    "model.get_layer('block2_conv2').trainable = False\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2294c3-9df0-4bb0-bd4b-3aef204ff10d",
   "metadata": {},
   "source": [
    "### Posts\n",
    "\n",
    "- [How to Improve Performance With Transfer Learning for Deep Learning Neural Networks](https://machinelearningmastery.com/how-to-improve-performance-with-transfer-learning-for-deep-learning-neural-networks/)\n",
    "- [A Gentle Introduction to Transfer Learning for Deep Learning](https://machinelearningmastery.com/transfer-learning-for-deep-learning/)\n",
    "- [How to Use The Pre-Trained VGG Model to Classify Objects in Photographs](https://machinelearningmastery.com/use-pre-trained-vgg-model-classify-objects-photographs/)\n",
    "\n",
    "### Books\n",
    "\n",
    "- [Deep Learning, 2016.](https://amzn.to/2NJW3gE)\n",
    "\n",
    "### Papers\n",
    "\n",
    "- [A Survey on Transfer Learning, 2010.](https://ieeexplore.ieee.org/document/5288526)\n",
    "- [How transferable are features in deep neural networks?, 2014.](https://arxiv.org/abs/1411.1792)\n",
    "- [CNN features off-the-shelf: An astounding baseline for recognition, 2014.](https://www.cv-foundation.org/openaccess/content_cvpr_workshops_2014/W15/html/Razavian_CNN_Features_Off-the-Shelf_2014_CVPR_paper.html)\n",
    "\n",
    "### APIs\n",
    "\n",
    "- [Keras Applications API](https://keras.io/applications/)\n",
    "\n",
    "### Articles\n",
    "\n",
    "- [Transfer Learning, Wikipedia.](https://en.wikipedia.org/wiki/Transfer_learning)\n",
    "- [Transfer Learning – Machine Learning’s Next Frontier, 2017.](http://ruder.io/transfer-learning/)\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this post, you discovered how to use transfer learning when developing convolutional neural networks for computer vision applications.\n",
    "\n",
    "Specifically, you learned:\n",
    "\n",
    "- Transfer learning involves using models trained on one problem as a starting point on a related problem.\n",
    "- Transfer learning is flexible, allowing the use of pre-trained models directly as feature extraction preprocessing and integrated into entirely new models.\n",
    "- Keras provides convenient access to many top performing models on the ImageNet image recognition tasks such as VGG, Inception, and ResNet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb05fb80-640a-42f7-85f4-5970c5159ee5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
