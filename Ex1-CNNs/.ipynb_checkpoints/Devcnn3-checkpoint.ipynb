{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0be730c6-0f88-46b5-b1ac-fac7559f646a",
   "metadata": {},
   "source": [
    "# 4.How to Develop Further Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0a2719-9435-4e7c-bf3a-c59fc468be3d",
   "metadata": {},
   "source": [
    "Learning has slowed down, so we will investigate increasing the number of training epochs to give the model enough space, if needed, to expose the learning dynamics in the learning curves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27a9e67-8453-44c3-91dc-e82b61ca563f",
   "metadata": {},
   "source": [
    "## Variation of Dropout Regularization:\n",
    "Dropout is working very well, so it may be worth investigating variations of how dropout is applied to the model.\n",
    "\n",
    "One variation that might be interesting is to increase the amount of dropout from 20% to 25% or 30%. Another variation that might be interesting is using a pattern of increasing dropout from 20% for the first block, 30% for the second block, and so on to 50% at the fully connected layer in the classifier part of the model.\n",
    "\n",
    "This type of increasing dropout with the depth of the model is a common pattern. It is effective as it forces layers deep in the model to regularize more than layers closer to the input.\n",
    "\n",
    "The baseline model with dropout updated to use a pattern of increasing dropout percentage with model depth is defined below.\n",
    "\n",
    "```python\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))\n",
    "\tmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "\tmodel.add(MaxPooling2D((2, 2)))\n",
    "\tmodel.add(Dropout(0.2))\n",
    "\tmodel.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "\tmodel.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "\tmodel.add(MaxPooling2D((2, 2)))\n",
    "\tmodel.add(Dropout(0.3))\n",
    "\tmodel.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "\tmodel.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "\tmodel.add(MaxPooling2D((2, 2)))\n",
    "\tmodel.add(Dropout(0.4))\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "\tmodel.add(Dropout(0.5))\n",
    "\tmodel.add(Dense(10, activation='softmax'))\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.001, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\treturn model\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d63a66c-8e7a-4610-8974-a8a076681614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 32, 32, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 16, 16, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16, 16, 32)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 16, 16, 64)        18496     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 16, 16, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 8, 8, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 8, 8, 128)         73856     \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 4, 4, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2048)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               262272    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 550,570\n",
      "Trainable params: 550,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "782/782 [==============================] - 11s 9ms/step - loss: 2.1822 - accuracy: 0.1735 - val_loss: 1.9823 - val_accuracy: 0.2715\n",
      "Epoch 2/50\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 1.8522 - accuracy: 0.2976 - val_loss: 1.6757 - val_accuracy: 0.3988\n",
      "Epoch 3/50\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 1.7086 - accuracy: 0.3581 - val_loss: 1.5481 - val_accuracy: 0.4369\n",
      "Epoch 4/50\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 1.6063 - accuracy: 0.4022 - val_loss: 1.4878 - val_accuracy: 0.4598\n",
      "Epoch 5/50\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 1.5281 - accuracy: 0.4369 - val_loss: 1.3676 - val_accuracy: 0.5061\n",
      "Epoch 6/50\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 1.4651 - accuracy: 0.4618 - val_loss: 1.3115 - val_accuracy: 0.5193\n",
      "Epoch 7/50\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 1.4171 - accuracy: 0.4831 - val_loss: 1.2757 - val_accuracy: 0.5425\n",
      "Epoch 8/50\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 1.3644 - accuracy: 0.5044 - val_loss: 1.2104 - val_accuracy: 0.5618\n",
      "Epoch 9/50\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 1.3253 - accuracy: 0.5189 - val_loss: 1.1688 - val_accuracy: 0.5791\n",
      "Epoch 10/50\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 1.2804 - accuracy: 0.5387 - val_loss: 1.1359 - val_accuracy: 0.5944\n",
      "Epoch 11/50\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 1.2409 - accuracy: 0.5526 - val_loss: 1.1402 - val_accuracy: 0.5939\n",
      "Epoch 12/50\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 1.2050 - accuracy: 0.5674 - val_loss: 1.0673 - val_accuracy: 0.6205\n",
      "Epoch 13/50\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 1.1727 - accuracy: 0.5805 - val_loss: 1.0189 - val_accuracy: 0.6385\n",
      "Epoch 14/50\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 1.1385 - accuracy: 0.5934 - val_loss: 0.9802 - val_accuracy: 0.6485\n",
      "Epoch 15/50\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 1.1133 - accuracy: 0.6045 - val_loss: 0.9816 - val_accuracy: 0.6470\n",
      "Epoch 16/50\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 1.0773 - accuracy: 0.6158 - val_loss: 0.9848 - val_accuracy: 0.6513\n",
      "Epoch 17/50\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 1.0638 - accuracy: 0.6231 - val_loss: 0.9653 - val_accuracy: 0.6582\n",
      "Epoch 18/50\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 1.0437 - accuracy: 0.6296 - val_loss: 0.9649 - val_accuracy: 0.6531\n",
      "Epoch 19/50\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 1.0205 - accuracy: 0.6354 - val_loss: 0.8980 - val_accuracy: 0.6759\n",
      "Epoch 20/50\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 1.0113 - accuracy: 0.6421 - val_loss: 0.8714 - val_accuracy: 0.6890\n",
      "Epoch 21/50\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.9912 - accuracy: 0.6471 - val_loss: 0.8553 - val_accuracy: 0.6928\n",
      "Epoch 22/50\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.9744 - accuracy: 0.6568 - val_loss: 0.8397 - val_accuracy: 0.7010\n",
      "Epoch 23/50\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.9573 - accuracy: 0.6605 - val_loss: 0.8304 - val_accuracy: 0.7015\n",
      "Epoch 24/50\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.9364 - accuracy: 0.6675 - val_loss: 0.8646 - val_accuracy: 0.6888\n",
      "Epoch 25/50\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.9285 - accuracy: 0.6742 - val_loss: 0.8028 - val_accuracy: 0.7179\n",
      "Epoch 26/50\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.9188 - accuracy: 0.6758 - val_loss: 0.8078 - val_accuracy: 0.7165\n",
      "Epoch 27/50\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.9034 - accuracy: 0.6840 - val_loss: 0.7895 - val_accuracy: 0.7207\n",
      "Epoch 28/50\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.8937 - accuracy: 0.6869 - val_loss: 0.7728 - val_accuracy: 0.7282\n",
      "Epoch 29/50\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.8873 - accuracy: 0.6906 - val_loss: 0.8218 - val_accuracy: 0.7072\n",
      "Epoch 30/50\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.8708 - accuracy: 0.6936 - val_loss: 0.7633 - val_accuracy: 0.7323\n",
      "Epoch 31/50\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.8593 - accuracy: 0.6975 - val_loss: 0.7727 - val_accuracy: 0.7271\n",
      "Epoch 32/50\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.8512 - accuracy: 0.6991 - val_loss: 0.7319 - val_accuracy: 0.7423\n",
      "Epoch 33/50\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.8373 - accuracy: 0.7062 - val_loss: 0.7456 - val_accuracy: 0.7323\n",
      "Epoch 34/50\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.8265 - accuracy: 0.7112 - val_loss: 0.7845 - val_accuracy: 0.7238\n",
      "Epoch 35/50\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.8176 - accuracy: 0.7131 - val_loss: 0.7037 - val_accuracy: 0.7550\n",
      "Epoch 36/50\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.8107 - accuracy: 0.7157 - val_loss: 0.7057 - val_accuracy: 0.7515\n",
      "Epoch 37/50\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.8026 - accuracy: 0.7191 - val_loss: 0.7198 - val_accuracy: 0.7456\n",
      "Epoch 38/50\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.7927 - accuracy: 0.7202 - val_loss: 0.6902 - val_accuracy: 0.7605\n",
      "Epoch 39/50\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.7812 - accuracy: 0.7285 - val_loss: 0.7330 - val_accuracy: 0.7380\n",
      "Epoch 40/50\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.7707 - accuracy: 0.7276 - val_loss: 0.6835 - val_accuracy: 0.7624\n",
      "Epoch 41/50\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.7670 - accuracy: 0.7308 - val_loss: 0.6934 - val_accuracy: 0.7563\n",
      "Epoch 42/50\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.7600 - accuracy: 0.7347 - val_loss: 0.6681 - val_accuracy: 0.7659\n",
      "Epoch 43/50\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.7513 - accuracy: 0.7367 - val_loss: 0.6604 - val_accuracy: 0.7679\n",
      "Epoch 44/50\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.7470 - accuracy: 0.7385 - val_loss: 0.6621 - val_accuracy: 0.7712\n",
      "Epoch 45/50\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.7378 - accuracy: 0.7422 - val_loss: 0.6636 - val_accuracy: 0.7693\n",
      "Epoch 46/50\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.7309 - accuracy: 0.7431 - val_loss: 0.6710 - val_accuracy: 0.7659\n",
      "Epoch 47/50\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.7258 - accuracy: 0.7463 - val_loss: 0.6473 - val_accuracy: 0.7742\n",
      "Epoch 48/50\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.7173 - accuracy: 0.7489 - val_loss: 0.6323 - val_accuracy: 0.7820\n",
      "Epoch 49/50\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.7115 - accuracy: 0.7511 - val_loss: 0.6422 - val_accuracy: 0.7780\n",
      "Epoch 50/50\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.6995 - accuracy: 0.7563 - val_loss: 0.6302 - val_accuracy: 0.7804\n",
      "> 78.040\n"
     ]
    }
   ],
   "source": [
    "## Full code system:\n",
    "# plot diagnostic learning curves\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import  Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import sys\n",
    "\n",
    "# load train and test dataset\n",
    "def load_dataset():\n",
    "    # load dataset\n",
    "    (trainX, trainY), (testX, testY) = cifar10.load_data()\n",
    "    # one hot encode target values\n",
    "    trainY = to_categorical(trainY)\n",
    "    testY = to_categorical(testY)\n",
    "    return trainX, trainY, testX, testY\n",
    "# scale pixels\n",
    "def prep_pixels(train, test):\n",
    "    # convert from integers to floats\n",
    "    train_norm = train.astype('float32')\n",
    "    test_norm = test.astype('float32')\n",
    "    # normalize to range 0-1\n",
    "    train_norm = train_norm / 255.0\n",
    "    test_norm = test_norm / 255.0\n",
    "    # return normalized images\n",
    "    return train_norm, test_norm\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    # compile model\n",
    "    opt = SGD(learning_rate=0.001, momentum=0.9)\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def summarize_diagnostics(history, nametrain=\"ass\"):\n",
    "    # plot loss\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.subplot(211)\n",
    "    plt.title('Cross Entropy Loss')\n",
    "    plt.plot(history.history['loss'], color='blue', label='train')\n",
    "    plt.plot(history.history['val_loss'], color='orange', label='test')\n",
    "    # plot accuracy\n",
    "    plt.subplot(212)\n",
    "    plt.title('Classification Accuracy')\n",
    "    plt.plot(history.history['accuracy'], color='blue', label='train')\n",
    "    plt.plot(history.history['val_accuracy'], color='orange', label='test')\n",
    "    # save plot to file\n",
    "    filename = \"Image/VGG_img{}\".format(nametrain)\n",
    "    plt.savefig(filename + '_plot.png')\n",
    "    plt.close()\n",
    "\n",
    "def main(nametrain):\n",
    "    # load dataset\n",
    "    trainX, trainY, testX, testY = load_dataset()\n",
    "    # prepare pixel data\n",
    "    trainX, testX = prep_pixels(trainX, testX)\n",
    "    # define model\n",
    "    model = define_model()\n",
    "    # fit model\n",
    "    history = model.fit(trainX, trainY, epochs=50, batch_size=64, validation_data=(testX, testY), verbose=1)\n",
    "    # evaluate model\n",
    "    _, acc = model.evaluate(testX, testY, verbose=0)\n",
    "    print('> %.3f' % (acc * 100.0))\n",
    "    # learning curves\n",
    "    summarize_diagnostics(history, nametrain)\n",
    "if __name__ == \"__main__\":\n",
    "    main(\"V3_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ad068c-7c5d-48b8-96f7-5a76a609188f",
   "metadata": {},
   "source": [
    "Reviewing the learning curves, we can see that the model converges well, with performance on the test dataset perhaps stalling at around 110 to 125 epochs. Compared to the learning curves for fixed dropout, we can see that again the rate of learning has been further slowed, allowing further refinement of the model without overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5292f0-6a82-47b6-9a0a-9deaee5612c5",
   "metadata": {},
   "source": [
    "## Dropout and Data Augmentation:\n",
    "In this section, we can experiment with combining both of these changes to the model to see if a further improvement can be achieved. Specifically, whether using both regularization techniques together results in better performance than either technique used alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "036735f4-2aeb-4bb9-ae1d-d7c1b6cd2437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_6 (Conv2D)           (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 32, 32, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 16, 16, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 16, 16, 32)        0         \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 16, 16, 64)        18496     \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 16, 16, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 8, 8, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 8, 8, 128)         73856     \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 8, 8, 128)         147584    \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 4, 4, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               262272    \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 550,570\n",
      "Trainable params: 550,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "781/781 [==============================] - 16s 20ms/step - loss: 2.1340 - accuracy: 0.2035 - val_loss: 1.8952 - val_accuracy: 0.3166\n",
      "Epoch 2/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 1.7933 - accuracy: 0.3384 - val_loss: 1.5894 - val_accuracy: 0.4295\n",
      "Epoch 3/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 1.6533 - accuracy: 0.3895 - val_loss: 1.4904 - val_accuracy: 0.4549\n",
      "Epoch 4/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 1.5649 - accuracy: 0.4250 - val_loss: 1.3788 - val_accuracy: 0.5013\n",
      "Epoch 5/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 1.4860 - accuracy: 0.4575 - val_loss: 1.3369 - val_accuracy: 0.5131\n",
      "Epoch 6/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 1.4247 - accuracy: 0.4813 - val_loss: 1.2832 - val_accuracy: 0.5349\n",
      "Epoch 7/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 1.3718 - accuracy: 0.4998 - val_loss: 1.2198 - val_accuracy: 0.5599\n",
      "Epoch 8/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 1.3101 - accuracy: 0.5279 - val_loss: 1.1654 - val_accuracy: 0.5848\n",
      "Epoch 9/50\n",
      "781/781 [==============================] - 15s 20ms/step - loss: 1.2674 - accuracy: 0.5404 - val_loss: 1.1133 - val_accuracy: 0.6000\n",
      "Epoch 10/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 1.2196 - accuracy: 0.5605 - val_loss: 1.0887 - val_accuracy: 0.6085\n",
      "Epoch 11/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 1.1702 - accuracy: 0.5789 - val_loss: 1.0220 - val_accuracy: 0.6329\n",
      "Epoch 12/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 1.1327 - accuracy: 0.5946 - val_loss: 1.0040 - val_accuracy: 0.6464\n",
      "Epoch 13/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 1.0983 - accuracy: 0.6100 - val_loss: 1.0331 - val_accuracy: 0.6310\n",
      "Epoch 14/50\n",
      "781/781 [==============================] - 16s 20ms/step - loss: 1.0613 - accuracy: 0.6209 - val_loss: 0.9778 - val_accuracy: 0.6553\n",
      "Epoch 15/50\n",
      "781/781 [==============================] - 15s 20ms/step - loss: 1.0351 - accuracy: 0.6312 - val_loss: 0.9086 - val_accuracy: 0.6761\n",
      "Epoch 16/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 1.0101 - accuracy: 0.6427 - val_loss: 0.8923 - val_accuracy: 0.6872\n",
      "Epoch 17/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 0.9845 - accuracy: 0.6524 - val_loss: 0.9071 - val_accuracy: 0.6792\n",
      "Epoch 18/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 0.9618 - accuracy: 0.6606 - val_loss: 0.8634 - val_accuracy: 0.6930\n",
      "Epoch 19/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 0.9379 - accuracy: 0.6670 - val_loss: 0.8667 - val_accuracy: 0.6992\n",
      "Epoch 20/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 0.9215 - accuracy: 0.6747 - val_loss: 0.8968 - val_accuracy: 0.6838\n",
      "Epoch 21/50\n",
      "781/781 [==============================] - 15s 20ms/step - loss: 0.9006 - accuracy: 0.6827 - val_loss: 0.8188 - val_accuracy: 0.7111\n",
      "Epoch 22/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 0.8883 - accuracy: 0.6865 - val_loss: 0.7663 - val_accuracy: 0.7284\n",
      "Epoch 23/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 0.8717 - accuracy: 0.6915 - val_loss: 0.8409 - val_accuracy: 0.7081\n",
      "Epoch 24/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 0.8594 - accuracy: 0.6973 - val_loss: 0.7477 - val_accuracy: 0.7382\n",
      "Epoch 25/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 0.8492 - accuracy: 0.7019 - val_loss: 0.7399 - val_accuracy: 0.7387\n",
      "Epoch 26/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 0.8347 - accuracy: 0.7047 - val_loss: 0.8288 - val_accuracy: 0.7130\n",
      "Epoch 27/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 0.8248 - accuracy: 0.7099 - val_loss: 0.7415 - val_accuracy: 0.7387\n",
      "Epoch 28/50\n",
      "781/781 [==============================] - 15s 20ms/step - loss: 0.8088 - accuracy: 0.7162 - val_loss: 0.7241 - val_accuracy: 0.7503\n",
      "Epoch 29/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 0.7997 - accuracy: 0.7185 - val_loss: 0.6965 - val_accuracy: 0.7548\n",
      "Epoch 30/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 0.7922 - accuracy: 0.7223 - val_loss: 0.7198 - val_accuracy: 0.7461\n",
      "Epoch 31/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 0.7819 - accuracy: 0.7252 - val_loss: 0.6872 - val_accuracy: 0.7617\n",
      "Epoch 32/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 0.7646 - accuracy: 0.7321 - val_loss: 0.6715 - val_accuracy: 0.7669\n",
      "Epoch 33/50\n",
      "781/781 [==============================] - 16s 20ms/step - loss: 0.7642 - accuracy: 0.7326 - val_loss: 0.7248 - val_accuracy: 0.7486\n",
      "Epoch 34/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 0.7529 - accuracy: 0.7370 - val_loss: 0.6896 - val_accuracy: 0.7585\n",
      "Epoch 35/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 0.7473 - accuracy: 0.7376 - val_loss: 0.7126 - val_accuracy: 0.7497\n",
      "Epoch 36/50\n",
      "781/781 [==============================] - 15s 20ms/step - loss: 0.7355 - accuracy: 0.7421 - val_loss: 0.6987 - val_accuracy: 0.7591\n",
      "Epoch 37/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 0.7287 - accuracy: 0.7458 - val_loss: 0.6588 - val_accuracy: 0.7686\n",
      "Epoch 38/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 0.7256 - accuracy: 0.7445 - val_loss: 0.6734 - val_accuracy: 0.7673\n",
      "Epoch 39/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 0.7152 - accuracy: 0.7498 - val_loss: 0.7367 - val_accuracy: 0.7460\n",
      "Epoch 40/50\n",
      "781/781 [==============================] - 15s 20ms/step - loss: 0.7093 - accuracy: 0.7538 - val_loss: 0.6417 - val_accuracy: 0.7725\n",
      "Epoch 41/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 0.7033 - accuracy: 0.7528 - val_loss: 0.6343 - val_accuracy: 0.7799\n",
      "Epoch 42/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 0.6938 - accuracy: 0.7575 - val_loss: 0.6500 - val_accuracy: 0.7713\n",
      "Epoch 43/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 0.6933 - accuracy: 0.7564 - val_loss: 0.6605 - val_accuracy: 0.7699\n",
      "Epoch 44/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 0.6857 - accuracy: 0.7602 - val_loss: 0.6286 - val_accuracy: 0.7810\n",
      "Epoch 45/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 0.6778 - accuracy: 0.7632 - val_loss: 0.6750 - val_accuracy: 0.7660\n",
      "Epoch 46/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 0.6673 - accuracy: 0.7666 - val_loss: 0.6145 - val_accuracy: 0.7832\n",
      "Epoch 47/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 0.6678 - accuracy: 0.7658 - val_loss: 0.6421 - val_accuracy: 0.7773\n",
      "Epoch 48/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 0.6688 - accuracy: 0.7663 - val_loss: 0.6030 - val_accuracy: 0.7873\n",
      "Epoch 49/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 0.6629 - accuracy: 0.7697 - val_loss: 0.6042 - val_accuracy: 0.7864\n",
      "Epoch 50/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 0.6530 - accuracy: 0.7723 - val_loss: 0.6428 - val_accuracy: 0.7813\n",
      "> 78.130\n"
     ]
    }
   ],
   "source": [
    "## Full code system:\n",
    "# plot diagnostic learning curves\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import  Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import sys\n",
    "\n",
    "# load train and test dataset\n",
    "def load_dataset():\n",
    "    # load dataset\n",
    "    (trainX, trainY), (testX, testY) = cifar10.load_data()\n",
    "    # one hot encode target values\n",
    "    trainY = to_categorical(trainY)\n",
    "    testY = to_categorical(testY)\n",
    "    return trainX, trainY, testX, testY\n",
    "# scale pixels\n",
    "def prep_pixels(train, test):\n",
    "    # convert from integers to floats\n",
    "    train_norm = train.astype('float32')\n",
    "    test_norm = test.astype('float32')\n",
    "    # normalize to range 0-1\n",
    "    train_norm = train_norm / 255.0\n",
    "    test_norm = test_norm / 255.0\n",
    "    # return normalized images\n",
    "    return train_norm, test_norm\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    # compile model\n",
    "    opt = SGD(learning_rate=0.001, momentum=0.9)\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def summarize_diagnostics(history, nametrain=\"ass\"):\n",
    "    # plot loss\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.subplot(211)\n",
    "    plt.title('Cross Entropy Loss')\n",
    "    plt.plot(history.history['loss'], color='blue', label='train')\n",
    "    plt.plot(history.history['val_loss'], color='orange', label='test')\n",
    "    # plot accuracy\n",
    "    plt.subplot(212)\n",
    "    plt.title('Classification Accuracy')\n",
    "    plt.plot(history.history['accuracy'], color='blue', label='train')\n",
    "    plt.plot(history.history['val_accuracy'], color='orange', label='test')\n",
    "    # save plot to file\n",
    "    filename = \"Image/VGG_img{}\".format(nametrain)\n",
    "    plt.savefig(filename + '_plot.png')\n",
    "    plt.close()\n",
    "\n",
    "def main(nametrain):\n",
    "    # load dataset\n",
    "    trainX, trainY, testX, testY = load_dataset()\n",
    "    print(len(trainX))\n",
    "    # prepare pixel data\n",
    "    trainX, testX = prep_pixels(trainX, testX)\n",
    "    # define model\n",
    "    model = define_model()\n",
    "    # create data generator \n",
    "    datagen = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip= True)\n",
    "    # prepare iterator\n",
    "    it_train = datagen.flow(trainX, trainY, batch_size=64)\n",
    "    steps = int(trainX.shape[0] / 64)\n",
    "    # fit model\n",
    "    history = model.fit(it_train,  steps_per_epoch=steps, epochs=50, validation_data=(testX, testY), verbose=1)\n",
    "    # # evaluate model\n",
    "    _, acc = model.evaluate(testX, testY, verbose=0)\n",
    "    print('> %.3f' % (acc * 100.0))\n",
    "    # # learning curves\n",
    "    summarize_diagnostics(history, nametrain)\n",
    "if __name__ == \"__main__\":\n",
    "    main(\"V3_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9625a99-184d-44db-bc15-10c8054473af",
   "metadata": {},
   "source": [
    "In this case, we can see that as we would have hoped, using both regularization techniques together has resulted in a further lift in model performance on the test set. In this case, combining fixed dropout with about 83% and data augmentation with about 84% has resulted in an improvement to about 85% classification accuracy.\n",
    "\n",
    "Reviewing the learning curves, we can see that the convergence behavior of the model is also better than either fixed dropout and data augmentation alone. Learning has been slowed without overfitting, allowing continued improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56ffe81-c5a0-457d-b9db-e7bea74b4b4f",
   "metadata": {},
   "source": [
    "## Dropout and Data Augmentation and Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236959c0-72ec-4461-afa8-a10dfdabceb7",
   "metadata": {},
   "source": [
    "First, we can increase the number of training epochs from 200 to 400, to give the model more of an opportunity to improve.\n",
    "\n",
    "Next, we can add [batch normalization](https://machinelearningmastery.com/how-to-accelerate-learning-of-deep-neural-networks-with-batch-normalization/) in an effort to stabilize the learning and perhaps accelerate the learning process. To offset this acceleration, we can increase the regularization by changing the dropout from a fixed pattern to an increasing pattern.\n",
    "\n",
    "```python\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))\n",
    "\tmodel.add(BatchNormalization())\n",
    "\tmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "\tmodel.add(BatchNormalization())\n",
    "\tmodel.add(MaxPooling2D((2, 2)))\n",
    "\tmodel.add(Dropout(0.2))\n",
    "\tmodel.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "\tmodel.add(BatchNormalization())\n",
    "\tmodel.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "\tmodel.add(BatchNormalization())\n",
    "\tmodel.add(MaxPooling2D((2, 2)))\n",
    "\tmodel.add(Dropout(0.3))\n",
    "\tmodel.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "\tmodel.add(BatchNormalization())\n",
    "\tmodel.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "\tmodel.add(BatchNormalization())\n",
    "\tmodel.add(MaxPooling2D((2, 2)))\n",
    "\tmodel.add(Dropout(0.4))\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "\tmodel.add(BatchNormalization())\n",
    "\tmodel.add(Dropout(0.5))\n",
    "\tmodel.add(Dense(10, activation='softmax'))\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.001, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\treturn model\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3201d02-df24-40ea-8f5a-bad21deab5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_12 (Conv2D)          (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 32, 32, 32)       128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 32, 32, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 32, 32, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 16, 16, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 16, 16, 32)        0         \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 16, 16, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 16, 16, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_15 (Conv2D)          (None, 16, 16, 64)        36928     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 16, 16, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 8, 8, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " conv2d_16 (Conv2D)          (None, 8, 8, 128)         73856     \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 8, 8, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_17 (Conv2D)          (None, 8, 8, 128)         147584    \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 8, 8, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 4, 4, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 128)               262272    \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 128)              512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 552,874\n",
      "Trainable params: 551,722\n",
      "Non-trainable params: 1,152\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "781/781 [==============================] - 16s 20ms/step - loss: 2.1276 - accuracy: 0.2995 - val_loss: 1.4793 - val_accuracy: 0.4659\n",
      "Epoch 2/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 1.6101 - accuracy: 0.4119 - val_loss: 1.4358 - val_accuracy: 0.4723\n",
      "Epoch 3/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 1.4950 - accuracy: 0.4541 - val_loss: 1.3646 - val_accuracy: 0.4970\n",
      "Epoch 4/50\n",
      "781/781 [==============================] - 15s 20ms/step - loss: 1.4191 - accuracy: 0.4814 - val_loss: 1.4298 - val_accuracy: 0.4770\n",
      "Epoch 5/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 1.3590 - accuracy: 0.5034 - val_loss: 1.3131 - val_accuracy: 0.5214\n",
      "Epoch 6/50\n",
      "781/781 [==============================] - 15s 20ms/step - loss: 1.3079 - accuracy: 0.5256 - val_loss: 1.2035 - val_accuracy: 0.5655\n",
      "Epoch 7/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 1.2509 - accuracy: 0.5468 - val_loss: 1.2669 - val_accuracy: 0.5464\n",
      "Epoch 8/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 1.2082 - accuracy: 0.5630 - val_loss: 1.1597 - val_accuracy: 0.5836\n",
      "Epoch 9/50\n",
      "781/781 [==============================] - 15s 20ms/step - loss: 1.1709 - accuracy: 0.5800 - val_loss: 1.1135 - val_accuracy: 0.6011\n",
      "Epoch 10/50\n",
      "781/781 [==============================] - 15s 20ms/step - loss: 1.1381 - accuracy: 0.5925 - val_loss: 1.0571 - val_accuracy: 0.6209\n",
      "Epoch 11/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 1.1042 - accuracy: 0.6058 - val_loss: 1.0257 - val_accuracy: 0.6293\n",
      "Epoch 12/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 1.0767 - accuracy: 0.6164 - val_loss: 1.1054 - val_accuracy: 0.6093\n",
      "Epoch 13/50\n",
      "781/781 [==============================] - 16s 20ms/step - loss: 1.0502 - accuracy: 0.6264 - val_loss: 1.0523 - val_accuracy: 0.6207\n",
      "Epoch 14/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 1.0319 - accuracy: 0.6326 - val_loss: 1.0969 - val_accuracy: 0.6115\n",
      "Epoch 15/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 1.0166 - accuracy: 0.6416 - val_loss: 1.0355 - val_accuracy: 0.6306\n",
      "Epoch 16/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 0.9901 - accuracy: 0.6499 - val_loss: 0.9531 - val_accuracy: 0.6570\n",
      "Epoch 17/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 0.9736 - accuracy: 0.6559 - val_loss: 1.1136 - val_accuracy: 0.6111\n",
      "Epoch 18/50\n",
      "781/781 [==============================] - 15s 20ms/step - loss: 0.9595 - accuracy: 0.6603 - val_loss: 0.9502 - val_accuracy: 0.6671\n",
      "Epoch 19/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 0.9460 - accuracy: 0.6657 - val_loss: 0.9791 - val_accuracy: 0.6555\n",
      "Epoch 20/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 0.9342 - accuracy: 0.6727 - val_loss: 0.9348 - val_accuracy: 0.6734\n",
      "Epoch 21/50\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.9209 - accuracy: 0.6768 - val_loss: 0.9435 - val_accuracy: 0.6722\n",
      "Epoch 22/50\n",
      "781/781 [==============================] - 15s 20ms/step - loss: 0.9049 - accuracy: 0.6812 - val_loss: 0.9119 - val_accuracy: 0.6773\n",
      "Epoch 23/50\n",
      "781/781 [==============================] - 15s 20ms/step - loss: 0.8917 - accuracy: 0.6844 - val_loss: 0.8562 - val_accuracy: 0.6995\n",
      "Epoch 24/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 0.8798 - accuracy: 0.6900 - val_loss: 0.8632 - val_accuracy: 0.6997\n",
      "Epoch 25/50\n",
      "781/781 [==============================] - 16s 20ms/step - loss: 0.8723 - accuracy: 0.6928 - val_loss: 0.7980 - val_accuracy: 0.7189\n",
      "Epoch 26/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 0.8617 - accuracy: 0.6978 - val_loss: 0.7870 - val_accuracy: 0.7229\n",
      "Epoch 27/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 0.8469 - accuracy: 0.7025 - val_loss: 0.7887 - val_accuracy: 0.7221\n",
      "Epoch 28/50\n",
      "781/781 [==============================] - 15s 20ms/step - loss: 0.8408 - accuracy: 0.7047 - val_loss: 0.8549 - val_accuracy: 0.7022\n",
      "Epoch 29/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 0.8284 - accuracy: 0.7099 - val_loss: 0.8550 - val_accuracy: 0.7052\n",
      "Epoch 30/50\n",
      "781/781 [==============================] - 15s 20ms/step - loss: 0.8247 - accuracy: 0.7107 - val_loss: 0.7987 - val_accuracy: 0.7207\n",
      "Epoch 31/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 0.8204 - accuracy: 0.7154 - val_loss: 0.7820 - val_accuracy: 0.7284\n",
      "Epoch 32/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 0.8073 - accuracy: 0.7193 - val_loss: 0.7389 - val_accuracy: 0.7397\n",
      "Epoch 33/50\n",
      "781/781 [==============================] - 15s 20ms/step - loss: 0.7966 - accuracy: 0.7220 - val_loss: 0.7550 - val_accuracy: 0.7369\n",
      "Epoch 34/50\n",
      "781/781 [==============================] - 15s 20ms/step - loss: 0.7935 - accuracy: 0.7221 - val_loss: 0.7359 - val_accuracy: 0.7374\n",
      "Epoch 35/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 0.7836 - accuracy: 0.7271 - val_loss: 0.7394 - val_accuracy: 0.7408\n",
      "Epoch 36/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 0.7783 - accuracy: 0.7277 - val_loss: 0.7591 - val_accuracy: 0.7373\n",
      "Epoch 37/50\n",
      "781/781 [==============================] - 16s 20ms/step - loss: 0.7726 - accuracy: 0.7297 - val_loss: 0.6965 - val_accuracy: 0.7553\n",
      "Epoch 38/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 0.7622 - accuracy: 0.7342 - val_loss: 0.7541 - val_accuracy: 0.7357\n",
      "Epoch 39/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 0.7562 - accuracy: 0.7369 - val_loss: 0.7320 - val_accuracy: 0.7468\n",
      "Epoch 40/50\n",
      "781/781 [==============================] - 15s 20ms/step - loss: 0.7480 - accuracy: 0.7394 - val_loss: 0.6984 - val_accuracy: 0.7581\n",
      "Epoch 41/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 0.7446 - accuracy: 0.7413 - val_loss: 0.6743 - val_accuracy: 0.7646\n",
      "Epoch 42/50\n",
      "781/781 [==============================] - 15s 20ms/step - loss: 0.7378 - accuracy: 0.7427 - val_loss: 0.7245 - val_accuracy: 0.7496\n",
      "Epoch 43/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 0.7367 - accuracy: 0.7451 - val_loss: 0.6830 - val_accuracy: 0.7611\n",
      "Epoch 44/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 0.7246 - accuracy: 0.7479 - val_loss: 0.7157 - val_accuracy: 0.7527\n",
      "Epoch 45/50\n",
      "781/781 [==============================] - 16s 20ms/step - loss: 0.7223 - accuracy: 0.7482 - val_loss: 0.6550 - val_accuracy: 0.7711\n",
      "Epoch 46/50\n",
      "781/781 [==============================] - 15s 20ms/step - loss: 0.7218 - accuracy: 0.7471 - val_loss: 0.7461 - val_accuracy: 0.7396\n",
      "Epoch 47/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 0.7099 - accuracy: 0.7542 - val_loss: 0.6746 - val_accuracy: 0.7644\n",
      "Epoch 48/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 0.7037 - accuracy: 0.7554 - val_loss: 0.6707 - val_accuracy: 0.7659\n",
      "Epoch 49/50\n",
      "781/781 [==============================] - 16s 20ms/step - loss: 0.7016 - accuracy: 0.7568 - val_loss: 0.6860 - val_accuracy: 0.7629\n",
      "Epoch 50/50\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 0.6937 - accuracy: 0.7607 - val_loss: 0.6202 - val_accuracy: 0.7879\n",
      "> 78.790\n"
     ]
    }
   ],
   "source": [
    "## Full code system:\n",
    "# plot diagnostic learning curves\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import  Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import sys\n",
    "\n",
    "# load train and test dataset\n",
    "def load_dataset():\n",
    "    # load dataset\n",
    "    (trainX, trainY), (testX, testY) = cifar10.load_data()\n",
    "    # one hot encode target values\n",
    "    trainY = to_categorical(trainY)\n",
    "    testY = to_categorical(testY)\n",
    "    return trainX, trainY, testX, testY\n",
    "# scale pixels\n",
    "def prep_pixels(train, test):\n",
    "    # convert from integers to floats\n",
    "    train_norm = train.astype('float32')\n",
    "    test_norm = test.astype('float32')\n",
    "    # normalize to range 0-1\n",
    "    train_norm = train_norm / 255.0\n",
    "    test_norm = test_norm / 255.0\n",
    "    # return normalized images\n",
    "    return train_norm, test_norm\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    # compile model\n",
    "    opt = SGD(learning_rate=0.001, momentum=0.9)\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def summarize_diagnostics(history, nametrain=\"ass\"):\n",
    "    # plot loss\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.subplot(211)\n",
    "    plt.title('Cross Entropy Loss')\n",
    "    plt.plot(history.history['loss'], color='blue', label='train')\n",
    "    plt.plot(history.history['val_loss'], color='orange', label='test')\n",
    "    # plot accuracy\n",
    "    plt.subplot(212)\n",
    "    plt.title('Classification Accuracy')\n",
    "    plt.plot(history.history['accuracy'], color='blue', label='train')\n",
    "    plt.plot(history.history['val_accuracy'], color='orange', label='test')\n",
    "    # save plot to file\n",
    "    filename = \"Image/VGG_img{}\".format(nametrain)\n",
    "    plt.savefig(filename + '_plot.png')\n",
    "    plt.close()\n",
    "\n",
    "def main(nametrain):\n",
    "    # load dataset\n",
    "    trainX, trainY, testX, testY = load_dataset()\n",
    "    print(len(trainX))\n",
    "    # prepare pixel data\n",
    "    trainX, testX = prep_pixels(trainX, testX)\n",
    "    # define model\n",
    "    model = define_model()\n",
    "    # create data generator \n",
    "    datagen = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip= True)\n",
    "    # prepare iterator\n",
    "    it_train = datagen.flow(trainX, trainY, batch_size=64)\n",
    "    steps = int(trainX.shape[0] / 64)\n",
    "    # fit model\n",
    "    history = model.fit(it_train,  steps_per_epoch=steps, epochs=50, validation_data=(testX, testY), verbose=1)\n",
    "    # # evaluate model\n",
    "    _, acc = model.evaluate(testX, testY, verbose=0)\n",
    "    print('> %.3f' % (acc * 100.0))\n",
    "    # # learning curves\n",
    "    summarize_diagnostics(history, nametrain)\n",
    "if __name__ == \"__main__\":\n",
    "    main(\"V3_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b1db6a-86cc-4845-99e4-cccf001d712e",
   "metadata": {},
   "source": [
    "In this case, we can see that we achieved a further lift in model performance to about 88% accuracy, improving upon both dropout and data augmentation alone at about 84% and upon the increasing dropout alone at about 85%.\n",
    "\n",
    "Reviewing the learning curves, we can see the training of the model shows continued improvement for nearly the duration of 400 epochs. We can see perhaps a slight drop-off on the test dataset at around 300 epochs, but the improvement trend does continue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbd22ac-1501-4415-ba2a-9ab53a498023",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "In this section, we explored two approaches designed to expand upon changes to the model that we know already result in an improvement\n",
    "\n",
    "A summary of the results is provided below:\n",
    "\n",
    "- Baseline + Increasing Dropout: 78.040%\n",
    "- Baseline + Dropout + Data Augmentation: 78.1300%\n",
    "- Baseline + Increasing Dropout + Data Augmentation + Batch Normalization: 78.790%\n",
    "\n",
    "The model is now learning well and we have good control over the rate of learning without overfitting.\n",
    "\n",
    "We might be able to achieve further improvements with additional regularization. This could be achieved with more aggressive dropout in later layers. It is possible that further addition of weight decay may improve the model.\n",
    "\n",
    "So far, we have not tuned the hyperparameters of the learning algorithm, such as the learning rate, which is perhaps the most important hyperparameter. We may expect further improvements with adaptive changes to the learning rate, such as use of an adaptive learning rate technique such as [Adam](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/). These types of changes may help to refine the model once converged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e343ebaf-9567-4854-952c-fc3c938e6844",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
