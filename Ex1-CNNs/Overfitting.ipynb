{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a13eb88-b5de-4dc7-a4b2-f719c09b920c",
   "metadata": {},
   "source": [
    "# Overfitting and Underfitting With Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d21af32-d0e8-4edf-a08b-8de36821917b",
   "metadata": {},
   "source": [
    "You learned about the terminology of generalization in machine learning of overfitting and underfitting:\n",
    "\n",
    "- **Overfitting**: Good performance on the training data, poor generliazation to other data.\n",
    "- **Underfitting**: Poor performance on the training data and poor generalization to other data.\n",
    "\n",
    "[link_here](https://machinelearningmastery.com/overfitting-and-underfitting-with-machine-learning-algorithms/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0767b03-7d7b-4d90-8900-f2c84fd0db12",
   "metadata": {},
   "source": [
    "# How to Avoid Overfitting in Deep Learning Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1dbacb-f9c2-4482-9b8b-178254bd1dc2",
   "metadata": {},
   "source": [
    "The model has not generalized.\n",
    "\n",
    "- **Underfit Model**. A model that fails to sufficiently learn the problem and performs poorly on a training dataset and does not perform well on a holdout sample.\n",
    "- **Overfit Model**. A model that learns the training dataset too well, performing well on the training dataset but does not perform well on a hold out sample.\n",
    "- **Good Fit Model**. A model that suitably learns the training dataset and generalizes well to the old out dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993f9f02-27e4-4d43-aa1e-9ab77f30c1d5",
   "metadata": {},
   "source": [
    "**An underfit model** has high bias and low variance. An overfit model has low bias and high variance. We can address underfitting by increasing the capacity of the model. Capacity refers to the ability of a model to fit a variety of functions; more capacity, means that a model can fit more types of functions for mapping inputs to outputs. Increasing the capacity of a model is easily achieved by changing the structure of the model, such as adding more layers and/or more nodes to layers.\n",
    "\n",
    "**An overfit model** is easily diagnosed by monitoring the performance of the model during training by evaluating it on both a training dataset and on a holdout validation dataset. Graphing line plots of the performance of the model during training, called learning curves, will show a familiar pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bed17db-0507-43c5-9851-684e830e42b9",
   "metadata": {},
   "source": [
    "## Reduce Overfitting by Constraining Model Complexity:\n",
    "There are two ways to approach an overfit model:\n",
    "\n",
    "1. Reduce overfitting by training the network on more examples.\n",
    "2. Reduce overfitting by changing the complexity of the network.\n",
    "\n",
    "A model can overfit a training dataset because it has sufficient capacity to do so. Reducing the capacity of the model reduces the likelihood of the model overfitting the training dataset, to a point where it no longer overfits.\n",
    "\n",
    "The capacity of a neural network model, it’s complexity, is defined by both it’s structure in terms of nodes and layers and the parameters in terms of its weights. Therefore, we can reduce the complexity of a neural network to reduce overfitting in one of two ways:\n",
    "\n",
    "1. Change network complexity by changing the network structure (number of weights).\n",
    "2. Change network complexity by changing the network parameters (values of weights).\n",
    "\n",
    "It is more common to instead constrain the complexity of the model by ensuring the parameters (weights) of the model remain small. Small parameters suggest a less complex and, in turn, more stable model that is less sensitive to statistical fluctuations in the input data.\n",
    "\n",
    "Techniques that seek to reduce overfitting (reduce generalization error) by keeping network weights small are referred to as regularization methods. More specifically, regularization refers to a class of approaches that add additional information to transform an ill-posed problem into a more stable well-posed problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ac55dc-6ed2-4901-8e8a-596f14ff82fa",
   "metadata": {},
   "source": [
    "## Regularization Methods for Neural Networks\n",
    "\n",
    "The most common regularization method is to add a penalty to the loss function in proportion to the size of the weights in the model.\n",
    "\n",
    "1. [Weight Regularization (weight decay)](https://machinelearningmastery.com/weight-regularization-to-reduce-overfitting-of-deep-learning-models/): Penalize the model during training based on the magnitude of the weights.\n",
    "\n",
    "This will encourage the model to map the inputs to the outputs of the training dataset in such a way that the weights of the model are kept small. This approach is called weight regularization or weight decay and has proven very effective for decades for both simpler linear models and neural networks.\n",
    "\n",
    "Below is a list of five of the most common additional regularization methods.\n",
    "\n",
    "1. [Activity Regularization](https://machinelearningmastery.com/how-to-reduce-generalization-error-in-deep-neural-networks-with-activity-regularization-in-keras/): Penalize the model during training base on the magnitude of the activations.\n",
    "2. [Weight Constraint](https://machinelearningmastery.com/introduction-to-weight-constraints-to-reduce-generalization-error-in-deep-learning/): Constrain the magnitude of weights to be within a range or below a limit.\n",
    "3. [Dropout](https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/): Probabilistically remove inputs during training.\n",
    "4. [Noise](https://machinelearningmastery.com/train-neural-networks-with-noise-to-reduce-overfitting/): Add statistical noise to inputs during training.\n",
    "5. [Early Stopping](https://machinelearningmastery.com/early-stopping-to-avoid-overtraining-neural-network-models/): Monitor model performance on a validation set and stop training when performance degrades.\n",
    "\n",
    "Most of these methods have been demonstrated (or proven) to approximate the effect of adding a penalty to the loss function.\n",
    "\n",
    "Each method approaches the problem differently, offering benefits in terms of a mixture of generalization performance, configurability, and/or computational complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06056367-0956-4172-bd1a-88856f3eb00f",
   "metadata": {},
   "source": [
    "## Regularization Recommendations\n",
    "This section outlines some recommendations for using regularization methods for deep learning neural networks.\n",
    "\n",
    "You should always consider using regularization, unless you have a very large dataset, e.g. big-data scale.\n",
    "\n",
    "A good general recommendation is to design a neural network structure that is under-constrained and to use regularization to reduce the likelihood of overfitting.\n",
    "\n",
    "Some more specific recommendations include:\n",
    "\n",
    "- Classical: use early stopping and weight decay (L2 weight regularization).\n",
    "- Alternate: use early stopping and added noise with a weight constraint.\n",
    "- Modern: use early stopping and dropout, in addition to a weight constraint.\n",
    "\n",
    "These recommendations would suit Multilayer Perceptrons and Convolutional Neural Networks.\n",
    "\n",
    "Some recommendations for recurrent neural nets include:\n",
    "\n",
    "- Classical: use early stopping with added weight noise and a weight constraint such as maximum norm.\n",
    "- Modern: use early stopping with a backpropagation-through-time-aware version of dropout and a weight constraint.\n",
    "\n",
    "There are no silver bullets when it comes to regularization and systematic experimentation is strongly encouraged."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a66e6e-f91d-49c1-a7d4-fb48cc74d2c9",
   "metadata": {},
   "source": [
    "[link_here](https://machinelearningmastery.com/introduction-to-regularization-to-reduce-overfitting-and-improve-generalization-error/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de94de84-88d5-4821-9eb3-f20e3834b62e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
